{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, Lars#, TweedieRegressor\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, PolynomialFeatures, OneHotEncoder, PowerTransformer\n",
    "from sklearn.model_selection import cross_validate, train_test_split, RepeatedKFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from vb_helper import VBHelper,shrinkBigKTransformer,logminus_T,exp_T,logminplus1_T,none_T, logp1_T,dropConst#,missingValHandler\n",
    "from missing_val_transformer import missingValHandler\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logdir=os.path.join(os.getcwd(),'log'); \n",
    "if not os.path.exists(logdir):os.mkdir(logdir)\n",
    "handlername=os.path.join(logdir,'vbflow.log')\n",
    "logging.basicConfig(\n",
    "    handlers=[logging.handlers.RotatingFileHandler(handlername, maxBytes=10**7, backupCount=100)],\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s [%(name)s.%(funcName)s:%(lineno)d] %(message)s\",\n",
    "    datefmt='%Y-%m-%dT%H:%M:%S')\n",
    "logger = logging.getLogger(handlername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import daal4py.sklearn\\ndaal4py.sklearn.patch_sklearn()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import daal4py.sklearn\n",
    "daal4py.sklearn.patch_sklearn()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_share=0.1\n",
    "cv_folds=10\n",
    "cv_reps=5\n",
    "cv_count=cv_folds*cv_reps\n",
    "rs=1 # random_state for reproducibility\n",
    "vbhelper=VBHelper(test_share,cv_folds,cv_reps,cv_count,rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STA_ID', 'LONG', 'LAT', 'OrigHabCode', 'Date', 'THG_Fish', 'YEAR', 'SEASON', 'SUBAREA', 'HABCODE', 'Floc_Depth_ft', 'AFDW_Floc', 'MEHG_Floc', 'THG_floc', 'Tot_Phos_floc', 'Bulk_Dens_Floc', 'Soil_Thickness_FT', 'AFDW_Soil', 'Bulk_Dens_Soil', 'PH_soil', 'SO4_soil', 'MEHG_soil', 'THG_soil', 'Tot_Carbon_Soil_%', 'Tot_Nitrogen_Soil_%', 'Tot_Phos_soil', 'Wat_Depth_ft', 'COND_SW', 'DO_SW', 'TEMP_SW', 'PH_SW', 'TURB_SW', 'REDOX_SW', 'Alk_Phos_SW', 'CHLA_SW', 'CL_SW', 'MEHG_SW', 'NH4_SW', 'NO2_SW', 'NO3_SW', 'SO4_SW', 'Sol_Reac_Phos_SW', 'THG_SW', 'TOC_SW', 'Tot_Nitrogen_SW', 'Tot_Phos_SW', 'REDOX_PW', 'H2S_PW', 'Sol_Reac_Phos_PW', 'MEHG_Peri_AVG', 'THG_epi_peri']\n"
     ]
    }
   ],
   "source": [
    "data_path=os.path.join('sample_data','ex1.csv')\n",
    "df=pd.read_csv(data_path)\n",
    "all_vars=list(df.columns)\n",
    "print(all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_name='THG_Fish'\n",
    "loc_vars=['LAT','LONG']\n",
    "drop_vars=['Date','OrigHabCode','STA_ID']\n",
    "drop_vars.extend(loc_vars)\n",
    "drop_vars.append(y_name)\n",
    "x_vars=[var for var in all_vars if var not in drop_vars]\n",
    "X_df=df.loc[:,x_vars]\n",
    "y_df=df.loc[:,y_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_intercept=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_share:\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=test_share, random_state=rs)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = (X_df, None, y_df, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,k=X_train.shape\n",
    "\n",
    "max_k=min([n//2,int(1.5*k)])\n",
    "\n",
    "vbhelper.max_k=max_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the strategy for handling missing values\n",
    "mis_val_strat='impute_middle' \n",
    "#'impute_middle' uses the mean for numerical and most frequent for categorical.\n",
    "# drop_row, impute_knn, impute_knn10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_regression_lars=lambda: Pipeline(steps=[\n",
    "    ('prep',missingValHandler(strategy=mis_val_strat)),\n",
    "    ('scale',StandardScaler()),\n",
    "    ('dropc',dropConst()),\n",
    "    ('shrink',shrinkBigKTransformer(max_k=max_k)),\n",
    "    ('linreg',LinearRegression(fit_intercept=include_intercept))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prep', missingValHandler(strategy='impute_middle')),\n",
       "                ('scale', StandardScaler()), ('dropc', dropConst()),\n",
       "                ('shrink', shrinkBigKTransformer(max_k=67, selector='Lars')),\n",
       "                ('linreg', LinearRegression(fit_intercept=1))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=pre_process_regression_lars()\n",
    "model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.490332335954495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.490332335954495"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.score(X_train,y_train))\n",
    "model.score(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lambda to make a callable object for creating new models, but with args set already\n",
    "# may be unnecessary due to sklearn cloning\n",
    "\n",
    "gridpoints=3 # grid points for gridsearchcv param_grid \n",
    "linear_regression=lambda: make_pipeline(missingValHandler(strategy=mis_val_strat),StandardScaler(),LinearRegression(fit_intercept=include_intercept)) \n",
    "linear_regression_lars=lambda: make_pipeline(missingValHandler(strategy=mis_val_strat),StandardScaler(),shrinkBigKTransformer(max_k=max_k),LinearRegression(fit_intercept=include_intercept)) #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html\n",
    "\n",
    "linear_svr =lambda: make_pipeline(missingValHandler(strategy=mis_val_strat),StandardScaler(),LinearSVR(fit_intercept=include_intercept,random_state=rs,tol=1e-4,max_iter=10000,C=1))\n",
    "rbf_svr=lambda: make_pipeline(missingValHandler(strategy=mis_val_strat),StandardScaler(),SVR(kernel='rbf',tol=1e-4,max_iter=5000, C=1))\n",
    "gradient_boosting_reg=lambda: make_pipeline(missingValHandler(strategy='pass-through'),HistGradientBoostingRegressor(max_iter=500))#,loss='poisson'))\n",
    "\n",
    "\n",
    "linear_svr = Pipeline(steps=[('pre_processor',missingValHandler(strategy=mis_val_strat)),('scaler',StandardScaler()),('lin_svr',LinearSVR(random_state=0,tol=1e-4,max_iter=50000))])\n",
    "lin_svr_param_grid={'lin_svr__C':np.logspace(-2,1,gridpoints)}\n",
    "linear_svr_cv=lambda: GridSearchCV(linear_svr,param_grid=lin_svr_param_grid)\n",
    "\n",
    "rbf_svr=Pipeline(steps=[('pre_processor',missingValHandler(strategy=mis_val_strat)),('scaler',StandardScaler()),('rbf_svr',SVR(kernel='rbf',tol=1e-4,max_iter=50000, cache_size=2*10**4))])\n",
    "rbf_svr_param_grid={'rbf_svr__C':np.logspace(-2,2,gridpoints),'rbf_svr__gamma':np.logspace(-1,0.5,gridpoints)} \n",
    "rbf_svr_cv=lambda: GridSearchCV(rbf_svr,param_grid=rbf_svr_param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "elastic_net =lambda: make_pipeline(missingValHandler(strategy=mis_val_strat),StandardScaler(), ElasticNet(fit_intercept=include_intercept))\n",
    "enet_params={\n",
    "    'elasticnet__alpha':np.logspace(-2,2,gridpoints),\n",
    "    'elasticnet__l1_ratio':np.linspace(0,1,gridpoints)\n",
    "}\n",
    "elastic_net_cv=lambda: GridSearchCV(elastic_net(),param_grid=enet_params)#,scoring=search_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=elastic_net_cv().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4912677256329574"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_list=[none_T(),logp1_T()]#[logminplus1_T(),none_T(),logminus_T()]#exp_T()] # imported...\n",
    "lin_reg_y_t_pipe=Pipeline(steps=[('ttr',TransformedTargetRegressor(regressor=linear_regression_lars()))])\n",
    "lin_reg_y_t_param_grid={\n",
    "    'ttr__transformer':transformer_list,\n",
    "    'ttr__regressor__shrinkbigktransformer__max_k':list(range(4,k,k//4)),\n",
    "\n",
    "}\n",
    "lin_reg_y_transform=lambda: GridSearchCV(lin_reg_y_t_pipe,param_grid=lin_reg_y_t_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cv',\n",
      " 'error_score',\n",
      " 'estimator',\n",
      " 'estimator__elasticnet',\n",
      " 'estimator__elasticnet__alpha',\n",
      " 'estimator__elasticnet__copy_X',\n",
      " 'estimator__elasticnet__fit_intercept',\n",
      " 'estimator__elasticnet__l1_ratio',\n",
      " 'estimator__elasticnet__max_iter',\n",
      " 'estimator__elasticnet__normalize',\n",
      " 'estimator__elasticnet__positive',\n",
      " 'estimator__elasticnet__precompute',\n",
      " 'estimator__elasticnet__random_state',\n",
      " 'estimator__elasticnet__selection',\n",
      " 'estimator__elasticnet__tol',\n",
      " 'estimator__elasticnet__warm_start',\n",
      " 'estimator__memory',\n",
      " 'estimator__missingvalhandler',\n",
      " 'estimator__missingvalhandler__strategy',\n",
      " 'estimator__missingvalhandler__transformer',\n",
      " 'estimator__standardscaler',\n",
      " 'estimator__standardscaler__copy',\n",
      " 'estimator__standardscaler__with_mean',\n",
      " 'estimator__standardscaler__with_std',\n",
      " 'estimator__steps',\n",
      " 'estimator__verbose',\n",
      " 'iid',\n",
      " 'n_jobs',\n",
      " 'param_grid',\n",
      " 'pre_dispatch',\n",
      " 'refit',\n",
      " 'return_train_score',\n",
      " 'scoring',\n",
      " 'verbose']\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "#pp.pprint(sorted(lin_reg_y_transform().get_params().keys()))\n",
    "pp.pprint(sorted(elastic_net_cv().get_params().keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add PolynomialFeatures() to gridsearch\n",
    "#### and try shrinking the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[\n",
    "    ('prep',missingValHandler(strategy=mis_val_strat)),\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('shrink_k1',shrinkBigKTransformer()), # retain a subset of the best original variables\n",
    "    ('polyfeat',PolynomialFeatures(interaction_only=0)), # create interactions among them\n",
    "    \n",
    "    ('drop_constant',dropConst()),\n",
    "    ('shrink_k2',shrinkBigKTransformer(selector=ElasticNet())), # pick from all of those options\n",
    "    ('reg',LinearRegression(fit_intercept=include_intercept))]\n",
    "\n",
    "\n",
    "X_T_pipe=Pipeline(steps=steps)\n",
    "inner_cv=RepeatedKFold(n_splits=10, n_repeats=1, random_state=rs)\n",
    " \n",
    "\n",
    "Y_T_X_T_pipe=Pipeline(steps=[('ttr',TransformedTargetRegressor(regressor=X_T_pipe))])\n",
    "Y_T__param_grid={\n",
    "    'ttr__transformer':transformer_list,\n",
    "    'ttr__regressor__polyfeat__degree':[2],\n",
    "    'ttr__regressor__shrink_k2__selector__alpha':np.logspace(-1,2,gridpoints),\n",
    "    'ttr__regressor__shrink_k2__selector__l1_ratio':np.linspace(0.2,.8,gridpoints),\n",
    "    'ttr__regressor__shrink_k1__max_k':[k//4,k//2,k],\n",
    "    #'ttr__regressor__prep__strategy':['impute_middle','impute_knn_5','drop_row']    \n",
    "}\n",
    "lin_reg_Xy_transform=lambda: GridSearchCV(Y_T_X_T_pipe,param_grid=Y_T__param_grid,cv=inner_cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_list=['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2'] #cross_validate wants strings\n",
    "search_scorer='r2'\n",
    "cv=RepeatedKFold(n_splits=cv_folds, n_repeats=cv_reps, random_state=rs) # define separately to ensure same cv data used for each model\n",
    "vbhelper.scorer_list=scorer_list\n",
    "# allow/generate water quality thresholds for stratified kfold sub-sampling to ensure cross-validation folds have full range of water quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42524733081681143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45515829319552015"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2=lin_reg_Xy_transform().fit(X_train,y_train)\n",
    "print(test2.score(X_train,y_train))\n",
    "test2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_dict={\n",
    "    #'linear-regression':linear_regression,\n",
    "    #'linear-regression-lars':linear_regression_lars,\n",
    "    'lin_reg_y_transform':lin_reg_y_transform,\n",
    "    'lin_reg_Xy_transform':lin_reg_Xy_transform,\n",
    "    'elastic-net':elastic_net_cv, \n",
    "    #'linear-svr-cv':linear_svr_cv, \n",
    "    #'rbf-svr-cv':rbf_svr_cv, \n",
    "    #'gradient-boosting-reg':gradient_boosting_reg\n",
    "   }\n",
    "vbhelper.estimator_dict=estimator_dict\n",
    "model_dict={key:val() for key,val in estimator_dict.items()} # they will be models once .fit is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin_reg_y_transform\n",
      "0.48342581370738247\n",
      "lin_reg_Xy_transform\n",
      "0.45515829319552015\n",
      "elastic-net\n",
      "0.4912677256329574\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for name,est in model_dict.items():\n",
    "    i+=1;print(name)\n",
    "    est.fit(X_train,y_train)\n",
    "    print(est.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin_reg_y_transform,[('neg_mean_squared_error', -7638.652710318391), ('neg_mean_absolute_error', -64.31697332928343), ('r2', 0.39507054890760285)]\n"
     ]
    }
   ],
   "source": [
    "cv_results={}\n",
    "for estimator_name,model in model_dict.items():\n",
    "    model_i=cross_validate(model, X_train, y_train, return_estimator=True, scoring=scorer_list, cv=cv, n_jobs=-1)\n",
    "    print(f\"{estimator_name},{[(scorer,np.mean(model_i[f'test_{scorer}'])) for scorer in scorer_list]}\")\n",
    "    cv_results[estimator_name]=model_i\n",
    "            \n",
    "# replace with a loop in order to save the residuals for a graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graphs and table to summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score_dict={}\n",
    "cv_score_dict_means={}\n",
    "for idx,(estimator_name,result) in enumerate(cv_results.items()):\n",
    "    #cv_estimators=result['estimator']\n",
    "    model_idx_scoredict={scorer:result[f'test_{scorer}'] for scorer in scorer_list}# fstring bc how cross_validate stores list of metrics\n",
    "    cv_score_dict[estimator_name]=model_idx_scoredict \n",
    "    model_idx_mean_scores={scorer:np.mean(scores) for scorer,scores in model_idx_scoredict.items()}\n",
    "    cv_score_dict_means[estimator_name]=model_idx_mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scorer in scorer_list:\n",
    "    print(f'scores for scorer: {scorer}:')\n",
    "    for estimator_name in model_dict:\n",
    "        print(f'    {estimator_name}:{cv_score_dict_means[estimator_name][scorer]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbhelper.plotCVScores(cv_score_dict,sort=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a similar plot showing residuals from the cv models for each value of y. \n",
    "# needs to be scatterplot or histogram since there will be (folds-1)*repeats predictions of each value of y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "### User chooses Linear Regression with LARS variable selection!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_estimator_name='linear-regression-lars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTestandCVScores(estimator_name,cv_score_dict_means):\n",
    "    model=estimator_dict[estimator_name]()\n",
    "    model.fit(X_train,y_train)\n",
    "    if test_share:\n",
    "        y_test_hat=model.predict(X_test)\n",
    "        print(f'test set: negative-mse={-mean_squared_error(y_test,y_test_hat)}')\n",
    "    for scorer in scorer_list:\n",
    "        print(f'cv avg: {scorer}= {cv_score_dict_means[estimator_name][scorer]}')\n",
    "    try:\n",
    "        print('coefficients: ',model[-1].coef_)\n",
    "        print('intercept: ',model[-1].intercept_)\n",
    "        #print('\\n','original positions: ',model[-2].col_select)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in estimator_dict.keys():\n",
    "    print(name)\n",
    "    printTestandCVScores(name,cv_score_dict_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printTestandCVScores('elastic-net',cv_score_dict_means)\n",
    "# fits better but soooo many coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printTestandCVScores('lin_reg_Xy_transform',cv_score_dict_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
